---
title: >
  Twitter Weekly Analysis –
  `r format(params$week_start, '%Y‑%m‑%d')` to
  `r format(params$week_start + 6, '%Y‑%m‑%d')`
author: ""
date: "`r Sys.Date()`"

params:
  week_start: !r lubridate::floor_date(Sys.Date(), unit = "week", week_start = 1)

output:
  html_document:
    toc: true
    toc_float: true
    number_sections: false
  pdf_document:
    latex_engine: xelatex
    toc: true
---


```{r, echo = FALSE,warning = FALSE,message = FALSE}

knitr::opts_chunk$set(
  echo = FALSE,   # hide code
  warning = FALSE,
  message = FALSE
)


# Load essential libraries
library(tidyverse)
library(lubridate)
library(tidytext)
library(stringi)
library(knitr)
library(kableExtra)
library(forcats)
library(widyr)
library(ggraph)
library(igraph)
library(data.table)
library(sentimentr)
```

```{r}
Sys.setenv(
  SUPABASE_HOST = "aws-0-us-east-2.pooler.supabase.com",   # exactly as shown
  SUPABASE_PORT = "6543",                                  # ← pooled port
  SUPABASE_DB   = "postgres",
  SUPABASE_USER = "postgres.kubvrwnqmsmhwcuscvje",         # note project ref!
  SUPABASE_PWD  = "hfa-tgt8nkj1AVM9vqe"
)

con <- DBI::dbConnect(
  RPostgres::Postgres(),
  host     = Sys.getenv("SUPABASE_HOST"),
  port     = as.integer(Sys.getenv("SUPABASE_PORT")),
  dbname   = Sys.getenv("SUPABASE_DB"),
  user     = Sys.getenv("SUPABASE_USER"),
  password = Sys.getenv("SUPABASE_PWD"),
  sslmode  = "require"
)

twitter_raw2 <- DBI::dbReadTable(con, "twitter_raw")

# ── 1. map handle → canonical user‑id ───────────────────────────
main_ids <- tibble::tribble(
  ~username,            ~main_id,
  "weave_db",           "1206153294680403968",
  "OdyseeTeam",         "1280241715987660801",
  "ardriveapp",         "1293193263579635712",
  "redstone_defi",      "1294053547630362630",
  "everpay_io",         "1334504432973848577",
  "decentlandlabs",     "1352388512788656136",
  "KYVENetwork",        "136377177683878784",
  "onlyarweave",        "1393171138436534272",
  "ar_io_network",      "1468980765211955205",
  "Permaswap",          "1496714415231717380",
  "communitylabs",      "1548502833401516032",
  "usewander",          "1559946771115163651",
  "apus_network",       "1569621659468054528",
  "fwdresearch",        "1573616135651545088",
  "perma_dao",          "1595075970309857280",
  "Copus_io",           "1610731228130312194",
  "basejumpxyz",        "1612781645588742145",
  "AnyoneFDN",          "1626376419268784130",
  "arweaveindia",       "1670147900033343489",
  "useload",            "1734941279379759105",
  "protocolland",       "1737805485326401536",
  "aoTheComputer",      "1750584639385939968",
  "ArweaveOasis",       "1750723327315030016",
  "aox_xyz",            "1751903735318720512",
  "astrousd",           "1761104764899606528",
  "PerplexFi",          "1775862139980226560",
  "autonomous_af",      "1777500373378322432",
  "Liquid_Ops",         "1795772412396507136",
  "ar_aostore",         "1797632049202794496",
  "FusionFiPro",        "1865790600462921728",
  "vela_ventures",      "1869466343000444928",
  "beaconwallet",       "1879152602681585664",
  "VentoSwap",          "1889714966321893376",
  "permawebjournal",    "1901592191065300993",
  "Botega_AF",          "1902521779161292800",
  "samecwilliams",      "409642632",
  "TateBerenbaum",      "801518825690824707",
  "ArweaveEco",         "892752981736779776"
)


# tweets_raw is the data frame you showed
tweets_tagged <- twitter_raw2 %>%                         # ← your df
  left_join(main_ids, by = "username") %>%
  # ── 2. classify rows ──────────────────────────────────────────
  mutate(
    is_rt_text = str_detect(text, "^RT @"),
    
    post_type = case_when(
      is_rt_text                                   ~ "retweet",
      user_id == main_id & is_rt_text == FALSE &
        str_detect(text, "https://t.co")           ~ "quote",      # rough proxy
      user_id == main_id                           ~ "original",
      TRUE                                         ~ "other"
    )
  )
```



```{r}

# 3) Add day, month, year, etc. for easy filtering
df <- tweets_tagged |>
  mutate(
    day      = as.Date(date),
    month    = lubridate::month(date),
    year     = lubridate::year(date),
    hour     = lubridate::hour(date),
    weekday  = lubridate::wday(date,
                               label  = TRUE,   # Mon, Tue, …
                               abbr   = FALSE,  # full names
                               locale = "en_US")) %>% 
  filter(post_type!="other")

# 3) Add day, month, year, etc. for easy filtering
# ── Define the 7‑day window -----------------------------------------------
week_start <- as.Date(params$week_start)           # Monday
week_end   <- week_start + lubridate::days(6)      # Sunday

df_week <- df %>%                                  # df comes from your earlier mutate()
  filter(date >= week_start,
         date <= week_end,
         post_type != "other") %>%
  mutate(
    week_day = lubridate::wday(date,
                               label  = TRUE, abbr = FALSE,
                               locale = "en_US"),
    week_hour = lubridate::hour(date)
  )

if (nrow(df_week) == 0){
  cat("\n\n### No tweets between",
      format(week_start, "%Y‑%m‑%d"), "and",
      format(week_end,   "%Y‑%m‑%d"), "– nothing to report.\n\n")
  knitr::knit_exit()
}

```


# Summary Table
```{r}
summary_table <- df_week %>%
  summarise(
    total_tweets = n(),
    avg_likes = mean(like_count, na.rm = TRUE),
    avg_comments = mean(reply_count, na.rm = TRUE),
    avg_impressions = mean(view_count, na.rm = TRUE),
    avg_engagement = mean(engagement_rate, na.rm = TRUE)
  )

summary_table %>%
  kbl(digits = 1, align = "c") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE,
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "white", background = "#000000") %>%
  add_header_above(c("Key Twitter Metrics – July 2025" = 5), bold = TRUE, align = "c")

```

# Top Keywords 

```{r}
# Custom stopwords
custom_stopwords <- tibble(word = c("ao", "aothecomputer", "rt", "https", "t.co", "1","2","3"))

all_stopwords <- bind_rows(stop_words, custom_stopwords)

# Tokenize
word_counts <- df_week %>%
  select(text) %>%
  unnest_tokens(word, text) %>%
  anti_join(all_stopwords, by = "word") %>%
  count(word, sort = TRUE) %>%
  slice_max(n, n = 20) # top 20

# Plot
word_counts %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 20 Most Common Words in Tweets",
       x = "Word", y = "Frequency") +
  theme_minimal()

```

# TF-IDF by Post Type 

```{r}
word_tfidf <- df_week %>%
  select(post_type, text) %>%
  filter(post_type!="other") %>% 
  mutate(
    text = str_remove_all(text, "http\\S+"),
    text = str_remove_all(text, "@\\w+"),
    text = str_remove_all(text, "[[:punct:]]"),
    text = stri_replace_all_regex(text, "[\\p{Emoji_Presentation}\\p{Extended_Pictographic}]+", "")
  ) %>%
  unnest_tokens(word, text) %>%
  filter(!str_detect(word, "^[0-9]+$"), word != "rt") %>%
  anti_join(stop_words, by = "word") %>%
  count(post_type, word, sort = TRUE) %>%
  bind_tf_idf(word, post_type, n) %>%
  group_by(post_type) %>%
  arrange(desc(tf_idf)) %>%
  mutate(rank = row_number()) %>%
  filter(rank <= 10) %>%
  ungroup()

word_tfidf %>%
  mutate(word = reorder_within(word, tf_idf, post_type)) %>%
  ggplot(aes(x = word, y = tf_idf, fill = post_type)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~post_type, scales = "free_y") +
  scale_x_reordered() +
  labs(
    title = "Top 10 Distinctive Words by Post Type",
    x = "Word",
    y = "TF-IDF Score"
  ) +
  coord_flip() +
  theme_minimal()

```

# Time-Based Analysis 
```{r}

Sys.setlocale("LC_TIME", "en_US.UTF-8")
# ------- 1. keep only tweets inside the 7‑day window ------------------------
df_week <- df %>%                                         # <- your master df
  filter(date >= week_start & date <  week_start + 7) %>% # Mon 00:00 – Mon 00:00
  mutate(day = as.Date(date, tz = "UTC"))                 # strip time → date

# ------- 2. daily tweet counts for that week --------------------------------
daily_counts <- df_week %>%
  count(day, name = "count")                              # same as group_by+summarise

# ------- 3. plot ------------------------------------------------------------
ggplot(daily_counts, aes(day, count)) +
  geom_line(color = "steelblue", linewidth = 1) +
  geom_point(color = "darkblue",  size = 2) +
  scale_x_date(
    breaks = seq(week_start, week_end, by = "1 day"),
    date_labels = "%a\n%d %b"
  ) +
  labs(
    title = sprintf(
      "Number of Tweets per Day",
      format(week_start, "%Y‑%m‑%d")
    ),
    x = NULL,
    y = "Tweet count"
  ) +
  theme_minimal()


```




```{r}
hourly_distribution <- df_week %>%
  mutate(hour = hour(date)) %>%  # Extract hour from datetime
  count(hour) %>%
  mutate(percentage = n / sum(n) * 100)

ggplot(hourly_distribution, aes(x = hour, y = percentage)) +
  geom_col(fill = "darkorange") +
  scale_x_continuous(breaks = 0:23) +
  labs(
    title = "Tweet Activity by Hour of Day",
    x = "Hour of Day",
    y = "Percentage of Tweets"
  ) +
  theme_minimal()

```





# Engagement Analysis

```{r}
engagement_by_day <- df_week %>%
  group_by(day) %>%
  summarise(avg_engagement_rate = mean(engagement_rate, na.rm = TRUE), .groups = "drop")

ggplot(engagement_by_day, aes(x = day, y = (avg_engagement_rate))) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(color = "darkblue", size = 2) +
  labs(
    title = "Average Engagement Rate Per Day",
    x = "Date",
    y = "Engagement Rate"
  ) +
  theme_minimal()
```


```{r}

engagement_by_day_type <- df_week %>%
  group_by(day, post_type) %>%
  summarise(avg_engagement_rate = mean(engagement_rate, na.rm = TRUE), .groups = "drop")

ggplot(engagement_by_day_type, aes(x = day, y = avg_engagement_rate, color = post_type)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(
    title = "Average Engagement Rate Per Day by Post Type",
    x = "Date",
    y = "Engagement Rate",
    color = "Post Type"
  ) +
  theme_minimal()

```




```{r}
engagement_by_type <- df_week %>%
  group_by(post_type) %>%
  summarise(avg_engagement_rate = mean(engagement_rate, na.rm = TRUE), .groups = "drop")

ggplot(engagement_by_type, aes(x = post_type, y = avg_engagement_rate, fill = post_type)) +
  geom_col() +
  labs(
    title = "Average Engagement Rate by Post Type",
    x = "Post Type",
    y = "Average Engagement Rate"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

```



# Comments Analysis

```{r}
comments_by_day <- df_week %>%
  group_by(day) %>%
  summarise(avg_comments = mean(reply_count, na.rm = TRUE), .groups = "drop")

ggplot(comments_by_day, aes(x = day, y = avg_comments)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(color = "darkblue", size = 2) +
  labs(
    title = "Average Comments (Replies) Per Day",
    x = "Date",
    y = "Average Comments"
  ) +
  theme_minimal()
```


```{r}

comments_by_day_type <- df_week %>%
  group_by(day, post_type) %>%
  summarise(avg_comments = mean(reply_count, na.rm = TRUE), .groups = "drop")

ggplot(comments_by_day_type, aes(x = day, y = avg_comments, color = post_type)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(
    title = "Average Comments Per Day by Post Type",
    x = "Date",
    y = "Average Comments",
    color = "Post Type"
  ) +
  theme_minimal()
```

```{r}
comments_by_type <- df_week %>%
  group_by(post_type) %>%
  summarise(avg_comments = mean(reply_count, na.rm = TRUE), .groups = "drop")

ggplot(comments_by_type, aes(x = post_type, y = avg_comments, fill = post_type)) +
  geom_col() +
  labs(
    title = "Average Comments by Post Type",
    x = "Post Type",
    y = "Average Comments"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

# Likes Analysis

```{r}
likes_by_day <- df_week %>%
  group_by(day) %>%
  summarise(avg_likes = mean(like_count, na.rm = TRUE), .groups = "drop")

ggplot(likes_by_day, aes(x = day, y = avg_likes)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(color = "darkblue", size = 2) +
  labs(
    title = "Average Likes Per Day",
    x = "Date",
    y = "Average Likes"
  ) +
  theme_minimal()
```

```{r}
likes_by_day_type <- df_week %>%
  group_by(day, post_type) %>%
  summarise(avg_likes = mean(like_count, na.rm = TRUE), .groups = "drop")

ggplot(likes_by_day_type, aes(x = day, y = avg_likes, color = post_type)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(
    title = "Average Likes Per Day by Post Type",
    x = "Date",
    y = "Average Likes",
    color = "Post Type"
  ) +
  theme_minimal()
```


```{r}

likes_by_type <- df_week %>%
  group_by(post_type) %>%
  summarise(avg_likes = mean(like_count, na.rm = TRUE), .groups = "drop")

ggplot(likes_by_type, aes(x = post_type, y = avg_likes, fill = post_type)) +
  geom_col() +
  labs(
    title = "Average Likes by Post Type",
    x = "Post Type",
    y = "Average Likes"
  ) +
  theme_minimal() +
  theme(legend.position = "none")


```


# Impressions Analysis

```{r}
views_by_day <- df_week %>%
  group_by(day) %>%
  summarise(avg_views = mean(view_count, na.rm = TRUE), .groups = "drop")

ggplot(views_by_day, aes(x = day, y = avg_views)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(color = "darkblue", size = 2) +
  labs(
    title = "Average Impressions Per Day",
    x = "Date",
    y = "Average Impressions"
  ) +
  theme_minimal()
```


```{r}
views_by_day_type <- df_week %>%
  group_by(day, post_type) %>%
  summarise(avg_views = mean(view_count, na.rm = TRUE), .groups = "drop")

ggplot(views_by_day_type, aes(x = day, y = avg_views, color = post_type)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(
    title = "Average Impressions Per Day by Post Type",
    x = "Date",
    y = "Average Impressions",
    color = "Post Type"
  ) +
  theme_minimal()
```


```{r}
views_by_type <- df_week %>%
  group_by(post_type) %>%
  summarise(avg_views = mean(view_count, na.rm = TRUE), .groups = "drop")

ggplot(views_by_type, aes(x = post_type, y = avg_views, fill = post_type)) +
  geom_col() +
  labs(
    title = "Average Impressions by Post Type",
    x = "Post Type",
    y = "Average Impressions"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```


# Average Engagement by Day and Hour (Log Scale)

```{r,message=F,warning=F}
Sys.setlocale("LC_TIME", "C") 

df_heatmap <- df_week %>%
  mutate(
    publish_dt = ymd_hms(date),                    # parse datetime
    day = lubridate::wday(publish_dt, label = TRUE),          # day of week (Sun, Mon, etc.)
    hour = lubridate::hour(publish_dt),                       # hour of day (0-23)
  ) %>%
  group_by(day, hour) %>%
  summarise(mean_engagement = mean(engagement_rate, na.rm = TRUE), .groups = "drop") %>%
  mutate(log_engagement = log1p(mean_engagement))  # log scale to handle spikes



# Plot
ggplot(df_heatmap, aes(x = hour, y = fct_rev(day), fill = log_engagement)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "C", name = "Log Engagement") +
  labs(
    title = "Average Engagement by Day and Hour (Log Scale)",
    x = "Hour of Day",
    y = "Day of Week"
  ) +
  theme_minimal()
```

# Top 10 Best Times to Post – Engagement Rate

```{r}

# Prepare the heatmap data (assuming df_week already exists)
df_heatmap <- df_week %>%
  mutate(
    publish_dt = ymd_hms(date),
    day = lubridate::wday(publish_dt, label = TRUE, abbr = FALSE, week_start = 1, locale = "C"),  # Force English
    hour = lubridate::hour(publish_dt)
  ) %>%
  group_by(day, hour) %>%
  summarise(mean_engagement = mean(engagement_rate, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(mean_engagement)) %>%
  slice_head(n = 10)

# View top 10 table
df_heatmap %>%
  knitr::kable(digits = 2, align = "c", caption = "Top 10 Best Times to Post – Engagement Rate") %>%
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE,
    position = "center"
  ) %>%
  kableExtra::column_spec(1:3, width = "5cm") %>%  # Adjust width of each column
  kableExtra::row_spec(0, bold = TRUE, color = "white", background = "#000000")


```


# Engagement by Hour and Post Type

```{r}

heatmap_post_hour <- df_week %>%
  mutate(hour = hour(date)) %>%
  group_by(post_type, hour) %>%
  summarise(mean_eng = median(engagement_rate, na.rm = TRUE), .groups = "drop")

ggplot(heatmap_post_hour, aes(x = hour, y = post_type, fill = mean_eng)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c() +
  labs(title = "Engagement by Hour and Post Type",
       x = "Hour", y = "Post Type", fill = "Median Engagement") +
  theme_minimal()

```



```{r}
# Tokenize and clean text
tokens <- df_week %>%
  select(tweet_id = tweet_id, text, engagement_rate) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!str_detect(word, "^\\d+$")) %>% 
    filter(!str_detect(word, "you're")) # remove numbers if needed


# ✅ Filter by word frequency (keep only words appearing in at least 5 tweets)
word_counts <- tokens %>%
  count(word) %>%
  filter(n >= 5)

tokens_filtered <- tokens %>%
  semi_join(word_counts, by = "word")

# Create binary word presence matrix
word_binary <- tokens_filtered %>%
  distinct(tweet_id, word) %>%
  mutate(present = 1) %>%
  pivot_wider(names_from = word, values_from = present, values_fill = 0)

# Join with engagement rate
engagement_words <- df_week %>%
  select(tweet_id = tweet_id, engagement_rate) %>%
  inner_join(word_binary, by = "tweet_id")

# Compute correlation for each word
correlations <- engagement_words %>%
  select(-tweet_id) %>%
  summarise(across(-engagement_rate, ~ cor(.x, engagement_rate, use = "complete.obs"))) %>%
  pivot_longer(cols = everything(), names_to = "word", values_to = "correlation") %>%
  arrange(desc(correlation)) %>%
  filter(!is.na(correlation))


  
```


# Top 10 Words Most Positively Correlated with Engagement Rate

```{r}

# Select top 10 correlated words
top_words <- correlations %>%
  filter(word!="you’re") %>% 
  slice_max(correlation, n = 10)

# Plot
ggplot(top_words, aes(x = reorder(word, correlation), y = correlation)) +
  geom_col(fill = "#00BFC4") +
  coord_flip() +
  labs(
    title = "Top 10 Words Most Positively Correlated with Engagement Rate",
    x = "Word",
    y = "Correlation"
  ) +
  theme_minimal()

```

# Top 10 Words Most Negatively Correlated with Engagement Rate

```{r}
# Select top 10 correlated words
top_words <- correlations %>%
  slice_min(correlation, n = 10)

# Plot
ggplot(top_words, aes(x = reorder(word, correlation), y = correlation)) +
  geom_col(fill = "#F8766D") +
  coord_flip() +
  labs(
    title = "Top 10 Words Most Negatively Correlated with Engagement Rate",
    x = "Word",
    y = "Correlation"
  ) +
  theme_minimal()
```


```{r}
# 1. Tokenize text into bigrams
bigrams <- df_week %>%
  select(tweet_id, text, engagement_rate) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))  # remove empty lines if any

# 2. Separate bigrams and remove stop words and "https" references
bigrams_separated <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(
    !word1 %in% stop_words$word,
    !word2 %in% stop_words$word,
    !str_detect(word1, "https"),
    !str_detect(word2, "https")
  ) %>%
  unite(bigram, word1, word2, sep = " ")

# 3. Filter by frequency (optional: only keep bigrams that occur 5+ times)
bigram_counts <- bigrams_separated %>%
  count(bigram) %>%
  filter(n >= 5)

bigrams_filtered <- bigrams_separated %>%
  semi_join(bigram_counts, by = "bigram")

# 4. Create binary presence matrix
bigram_binary <- bigrams_filtered %>%
  distinct(tweet_id, bigram) %>%
  mutate(present = 1) %>%
  pivot_wider(names_from = bigram, values_from = present, values_fill = 0)

# 5. Join with engagement rate
engagement_bigrams <- df_week %>%
  select(tweet_id, engagement_rate) %>%
  inner_join(bigram_binary, by = "tweet_id")

# 6. Compute correlations
bigram_correlations <- engagement_bigrams %>%
  select(-tweet_id) %>%
  summarise(across(-engagement_rate, ~ cor(.x, engagement_rate, use = "complete.obs"))) %>%
  pivot_longer(cols = everything(), names_to = "bigram", values_to = "correlation") %>%
  filter(!is.na(correlation)) %>%
  arrange(desc(correlation))


```


# Top 10 Bigrams Most Positively Correlated with Engagement


```{r}
top_positive_bigrams <- bigram_correlations %>%
  slice_max(correlation, n = 10)

ggplot(top_positive_bigrams, aes(x = reorder(bigram, correlation), y = correlation)) +
  geom_col(fill = "#00BFC4") +
  coord_flip() +
  labs(
    title = "Top 10 Bigrams Most Positively Correlated with Engagement",
    x = "Bigram",
    y = "Correlation"
  ) +
  theme_minimal()

```



# Top 10 Bigrams Most Negatively Correlated with Engagement

```{r}
top_negative_bigrams <- bigram_correlations %>%
  filter(!str_detect(bigram, "^rt\\b")) %>% 
  slice_min(correlation, n = 10)

ggplot(top_negative_bigrams, aes(x = reorder(bigram, correlation), y = correlation)) +
  geom_col(fill = "#F8766D") +
  coord_flip() +
  labs(
    title = "Top 10 Bigrams Most Negatively Correlated with Engagement",
    x = "Bigram",
    y = "Correlation"
  ) +
  theme_minimal()

```




# Top Hashtags by Engagement Rate

```{r}
# Extract hashtags
hashtags <- df_week %>%
  mutate(hashtags = str_extract_all(text, "#\\w+")) %>%
  unnest(hashtags) %>%
  group_by(hashtags) %>%
  summarise(avg_engagement = mean(engagement_rate, na.rm = TRUE), n = n()) %>%
  filter(n >= 5) %>%
  arrange(desc(avg_engagement)) %>%
  slice_head(n = 10)

# Plot
ggplot(hashtags, aes(x = reorder(hashtags, avg_engagement), y = avg_engagement)) +
  geom_col(fill = "purple") +
  coord_flip() +
  labs(title = "Top Hashtags by Engagement Rate",
       x = "Hashtag", y = "Avg Engagement Rate") +
  theme_minimal()

```


# Engagement Rate Distribution by Post Type

```{r}
ggplot(df_week, aes(x = post_type, y = log(engagement_rate), fill = post_type)) +
  geom_boxplot(outlier.color = "red", alpha = 0.7) +
  labs(title = "Engagement Rate Distribution by Post Type",
       x = "Post Type", y = "Engagement Rate") +
  theme_minimal() +
  theme(legend.position = "none")

```

# Top 30 Words Most Positively Correlated with Each Other

```{r}


# Assuming your dataset is already loaded as `df`
# and that the text column is called `text` and tweet_id is `tweet_id`

# Step 1: Tokenize and clean
tokens <- df_week %>%
  select(tweet_id, text) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word") %>%        # Remove common stopwords
  filter(!str_detect(word, "^\\d+$")) %>%       # Remove pure numbers
  filter(!str_detect(word, "^https"))           # Remove URLs

# Step 2: Filter for commonly used words
common_words <- tokens %>%
  count(word) %>%
  filter(n >= 15)                                # Adjust this threshold if needed

tokens_filtered <- tokens %>%
  semi_join(common_words, by = "word")

# Step 3: Compute pairwise correlation across tweets
word_correlations <- tokens_filtered %>%
  pairwise_cor(item = word, feature = tweet_id, sort = TRUE)


cleaned_word_pairs <- word_correlations %>%
  mutate(
    word_a = pmin(item1, item2),
    word_b = pmax(item1, item2)
  ) %>%
  select(word_a, word_b, correlation) %>%
  distinct() %>%
  arrange(desc(correlation))

# View top correlations
cleaned_word_pairs %>%
  filter(correlation > 0.35) %>%
  slice_max(correlation, n = 20) %>%
  knitr::kable(
    digits = 2,
    align = "c",
    caption = "Top 30 Words Most Positively Correlated with Each Other"
  ) %>%
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE,
    position = "center"
  ) %>%
  kableExtra::column_spec(1:2, width = "5cm") %>%  # Adjust columns as needed
  kableExtra::row_spec(0, bold = TRUE, color = "white", background = "#000000")


```







# Top 20 Distinctive Keywords by Engagement Tier

```{r}
## ── A.  add an engagement tier ──────────────────────────────────────────────
df_week <- df_week %>% 
  mutate(
    tier = cut(
      engagement_rate,
      breaks = quantile(engagement_rate, c(0, .33, .66, 1), na.rm = TRUE),
      labels = c("Low", "Medium", "High"),
      include.lowest = TRUE
    )
  )

## -- unigrams ---------------------------------------------------------------
uni <- df_week %>% 
  select(tier, text) %>% 
  unnest_tokens(word, text, token = "words")

## -- bigrams ---------------------------------------------------------------
bi <- df_week %>% 
  select(tier, text) %>% 
  unnest_tokens(word, text, token = "ngrams", n = 2) %>% 
  separate_rows(word, sep = " ")          # break “launch beta” → “launch” “beta”

## -- combine & clean --------------------------------------------------------
tidy_tokens <- bind_rows(uni, bi) %>% 
  filter(
    !word %in% c("https", "t.co", "rt"),   # remove URL / RT junk
    !str_detect(word, "^\\d+$")            # remove pure numbers
  ) %>% 
  anti_join(stop_words, by = "word")

## ── C.  use tf-idf to surface words *distinctive* for each tier  ────────────
tier_keywords <- tidy_tokens %>% 
  count(tier, word, sort = TRUE) %>% 
  bind_tf_idf(word, tier, n) %>% 
  group_by(tier) %>% 
  slice_max(tf_idf, n = 20, with_ties = FALSE)

# 6. Plot
ggplot(tier_keywords, aes(x = reorder_within(word, tf_idf, tier), y = tf_idf, fill = tier)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ tier, scales = "free_y") +
  scale_x_reordered() +
  labs(
    title = "Top 20 Distinctive Keywords by Engagement Tier",
    subtitle = "Words ranked by TF-IDF score",
    x = "Keyword",
    y = "TF-IDF Score"
  ) +
  theme_minimal() +
  coord_flip()
```


# Sentiment Analysis

```{r}
df_week1 <- df_week %>%
    as_tibble() %>%
    mutate(text = str_replace_all(text, "&quot;|&#x2F;", "'"),    
           text = str_replace_all(text, "&#x2F;", "/"),           
           text = str_replace_all(text, "<a(.*?)>", " "),          
           text = str_replace_all(text, "&gt;|&lt;", " "),        
           text = str_replace_all(text, "<[^>]*>", " "),
           text = str_replace_all(text,"http.*\\s*"," "),
           text = str_replace_all(text,"\\031","'"),
           text = str_replace_all(text,"\\[",""),
           text = str_replace_all(text,"\\[",""),
           text = str_replace_all(text,"\\]",""),
           text = str_replace_all(text,"\\(",""),
           text = str_replace_all(text,"\\)",""),
           text = str_replace_all(text,"\\*",""),
           text = str_replace_all(text,'"',""),
           text = str_replace_all(text,"'",""),
           text = str_replace_all(text,"~",""),
           text = str_replace_all(text,"#",""),
           text = str_replace_all(text,"\\n"," "),
           text = str_replace_all(text,"\\034",""),
           text = str_replace_all(text,"\\035",""),
           text = str_replace_all(text,"\\tI",""),
           text = str_replace_all(text,"_",""),
           text = str_replace_all(text,"\\\\"," "),
           text = str_replace_all(text,"a°",""),
           text = str_replace_all(text,"<",""),
           text = str_replace_all(text,">",""),
           text = str_replace_all(text,"&amp;",""),
           text = str_replace_all(text,"\\024",""),
           text = str_replace_all(text,"-",""),
           text = str_replace_all(text,"U\\+0096",""),
           text = str_replace_all(text,"\\033","")
           )   

df_week1<-df_week1 %>% mutate(words=str_count(text,"\\w+")) %>% as_tibble() %>%filter(words>5)

```

```{r}
library(sentimentr)
```


```{r}
polarity_dt = lexicon::hash_sentiment_jockers_rinker

sentiment_key <- lexicon::hash_sentiment_jockers_rinker %>% 
  dplyr::filter(!x %in% c("damn","damned","dammit","goddamn","goddamned","hell","hells","heck","jail","spoiler","goblin","hot","shot","fuck","fucked","fucks","mindfuck"))
```



```{r}

custom_words<-data.table(
  x = c("hot" ),
  y = c(1),
  key = "x")

sentiment_key2<-rbind(polarity_dt,custom_words)

sentiment_key2<-update_key(polarity_dt, x = data.frame(x = c("hot"), y = c(1)))

```



```{r}
mycomment <- get_sentences(df_week1$text)

sentiments3<-sentiment_by(mycomment,polarity_dt = sentiment_key2)
```

```{r}

df_week1 <- df_week1 %>%
  mutate(element_id = seq_len(nrow(df_week1)))

```

```{r}
df_week1<-inner_join(sentiments3,df_week1)
```
```{r}

df_week1 %>%
  ggplot(aes(x = "", y = ave_sentiment)) +
  geom_boxplot(fill = "#2C3E50", color = "black", width = 0.3, outlier.color = "red", outlier.shape = 16) +
  labs(
    title = "Distribution of Average Sentiment Scores",
    y = "Average Sentiment",
    x = NULL
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title.y = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.grid.major.y = element_line(color = "gray90"),
    panel.grid.major.x = element_blank()
  )+coord_flip()

```


```{r}
# Classify sentiment
df_week1 <- df_week1 %>%
  mutate(sentiment = case_when(
    ave_sentiment > 0.05 ~ "positive",
    ave_sentiment < -0.05 ~ "negative",
    TRUE ~ "neutral"
  ))

# Group by date (day) instead of month
daily_sentiment <- df_week1 %>%
  group_by(day, sentiment) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(day) %>%
  mutate(percent = count / sum(count) * 100)

# Plot
ggplot(daily_sentiment, aes(x = day, y = percent, color = sentiment)) +
  geom_line() +
  labs(
    title = "Daily Sentiment Trend on Twitter",
    x = "Date",
    y = "Percentage of Tweets",
    color = "Sentiment"
  ) +
  scale_color_manual(values = c("negative" = "red", "neutral" = "gray", "positive" = "blue")) +
  theme_minimal(base_size = 14)
```


```{r}
# Overall sentiment count
overall_sentiment <- df_week1 %>%
  count(sentiment) %>%
  mutate(percent = n / sum(n) * 100)

# Plot
ggplot(overall_sentiment, aes(x = sentiment, y = percent, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  scale_fill_manual(values = c("negative" = "tomato", "neutral" = "khaki", "positive" = "skyblue")) +
  labs(
    title = "Overall Sentiment Distribution on Twitter",
    x = "Sentiment Type",
    y = "Percentage of Tweets"
  ) +
  theme_minimal(base_size = 14)

```

```{r}
# Count sentiment within each post type and calculate percentage
sentiment_by_type <- df_week1 %>%
  count(post_type, sentiment) %>%
  group_by(post_type) %>%
  mutate(percent = n / sum(n) * 100) %>%
  ungroup()

# Plot
ggplot(sentiment_by_type, aes(x = sentiment, y = percent, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ post_type) +
  scale_fill_manual(values = c("negative" = "tomato", "neutral" = "khaki", "positive" = "skyblue")) +
  labs(
    title = "Sentiment Distribution by Tweet Type",
    x = "Sentiment Type",
    y = "Percentage of Tweets"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    axis.title = element_text(face = "bold")
  )

```





```{r}
# Calculate average sentiment by day
daily_avg_sentiment <- df_week1 %>%
  group_by(day) %>%
  summarise(avg_sentiment = mean(ave_sentiment, na.rm = TRUE))

# Plot
ggplot(daily_avg_sentiment, aes(x = day, y = avg_sentiment)) +
  geom_line(color = "steelblue") +
  labs(
    title = "Average Sentiment Score by Day",
    x = "Date",
    y = "Average Sentiment") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    axis.title = element_text(face = "bold")
  )
```


```{r}
# Calculate daily average sentiment by post type
daily_sentiment_by_type <- df_week1 %>%
  group_by(day, post_type) %>%
  summarise(avg_sentiment = mean(ave_sentiment, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(daily_sentiment_by_type, aes(x = day, y = avg_sentiment, color = post_type)) +
  geom_line() +
  labs(
    title = "Average Sentiment Score by Day and Post Type",
    x = "Date",
    y = "Average Sentiment Score",
    color = "Post Type"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    axis.title = element_text(face = "bold"),
    legend.position = "top"
  )

```



```{r}

nrc_key <- lexicon::hash_nrc_emotions %>% 
  dplyr::filter(
  emotion %in% c('trust',"anger","anticipation","fear","sadness","surprise","disgust","joy")) %>% 
  dplyr::filter(!token %in% c("damn","damned","dammit","goddamn","goddamned","peach","curse","cursed","hell","hells","heck","fuck","fucked","fucks","mindfuck","punch","money","insane","crazy","shit")) %>% filter(emotion!="fear"|token!="cringe") %>% 
filter(emotion!="sadness"|token!="cringe") 
```


```{r}
emotions2<-emotion_by(df_week1$text,emotion_dt =nrc_key)


df_week2<-inner_join(df_week1,emotions2,by="element_id")

```


```{r}
x<-df_week2%>% pivot_wider(names_from = emotion_type, values_from = ave_emotion) %>%
    mutate(across(c(anger:trust_negated), ~ifelse(is.na(.x), 0,.))) %>% mutate(anger=anger-anger_negated,anticipation=anticipation-anticipation_negated,disgust=disgust-disgust_negated,fear=fear-fear_negated,joy=joy-joy_negated,sadness=sadness-sadness_negated,surprise=surprise-surprise_negated,trust=trust-trust_negated) %>% pivot_longer(cols = anger:trust_negated, names_to = "emotion_type", values_to = "ave_emotion")%>%
  group_by(emotion_type) %>% 
  summarize(ave_emotion=mean(ave_emotion)) %>% pivot_wider(names_from = emotion_type, values_from = ave_emotion) %>% arrange(desc(anger))

y<-df_week2%>% pivot_wider(names_from = emotion_type, values_from = ave_emotion) %>%
    mutate(across(c(anger:trust_negated), ~ifelse(is.na(.x), 0,.))) %>% mutate(anger=anger-anger_negated,anticipation=anticipation-anticipation_negated,disgust=disgust-disgust_negated,fear=fear-fear_negated,joy=joy-joy_negated,sadness=sadness-sadness_negated,surprise=surprise-surprise_negated,trust=trust-trust_negated) %>% pivot_longer(cols = anger:trust_negated, names_to = "emotion_type", values_to = "ave_emotion")
```





```{r}
# Scatter plot: Sentiment Polarity vs Engagement Rate
ggplot(y, aes(x = (ave_sentiment), y = log(engagement_rate))) +
  geom_point(alpha = 0.6, color = "#2C3E50") +  # semi-transparent points
  geom_smooth(method = "lm", se = TRUE, color = "#E74C3C", linetype = "dashed") + # regression line
  labs(
    title = "Relationship between Sentiment Polarity and Engagement Rate",
    x = "Average Sentiment Polarity",
    y = "Engagement Rate"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    axis.title = element_text(face = "bold")
  )
```

## Emotion Analysis

```{r}

# 1. Filter: Remove negated emotions AND ave_emotion <= 0
y_filtered <- y %>%
  filter(!str_detect(emotion_type, "negated$")) %>%
  filter(ave_emotion > 0)

# 2. Daily trend
daily_emotion <- y_filtered %>%
  group_by(day, emotion_type) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(day) %>%
  mutate(percent = count / sum(count) * 100)

# 3. Plot
ggplot(daily_emotion, aes(x = day, y = percent, color = emotion_type)) +
  geom_line(size = 0.5) +
  labs(
    title = "Daily Emotion Trend",
    x = "Date",
    y = "Percentage of Tweets",
    color = "Emotion"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    axis.title = element_text(face = "bold")
  )

```





```{r}
# 4. Overall
overall_emotion <- y_filtered %>%
  count(emotion_type) %>%
  mutate(percent = n / sum(n) * 100)

# 5. Plot
ggplot(overall_emotion, aes(x = reorder(emotion_type, percent), y = percent, fill = emotion_type)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Overall Emotion Distribution",
    x = "Emotion",
    y = "Percentage of Tweets"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    axis.title = element_text(face = "bold")
  )

```


```{r}
# 6. Emotion per post type
emotion_by_post_type <- y_filtered %>%
  count(post_type, emotion_type) %>%
  group_by(post_type) %>%
  mutate(percent = n / sum(n) * 100) %>%
  ungroup()


# 7. Plot
ggplot(emotion_by_post_type, aes(x = emotion_type, y = percent, fill = emotion_type)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ post_type) +
  labs(
    title = "Emotion Distribution by Tweet Type",
    x = "Emotion",
    y = "Percentage of Tweets"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    axis.title = element_text(face = "bold"),
    strip.text = element_text(face = "bold")
  ) +
  coord_flip()

```

















